{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *B*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Keijiro Tajima*\n",
    "* *Mahammad Shirinov*\n",
    "* *Stephen Zhao*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl, save_pkl\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer as PS\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "courses_original = load_json('data/courses.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of courses is 854.\n"
     ]
    }
   ],
   "source": [
    "# Check the number of courses\n",
    "print(\"The number of courses is {}.\".format(len(courses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE-440\n",
      "The latest developments in processing and the novel generations of organic composites are discussed. Nanocomposites, adaptive composites and biocomposites are presented. Product development, cost analysis and study of new markets are practiced in team work. Content Basics of composite materialsConstituentsProcessing of compositesDesign of composite structures Current developmentNanocomposites Textile compositesBiocompositesAdaptive composites ApplicationsDriving forces and marketsCost analysisAerospaceAutomotiveSport Keywords Composites - Applications - Nanocomposites - Biocomposites - Adaptive composites - Design - Cost Learning Prerequisites Required courses Notion of polymers Recommended courses Polymer Composites Learning Outcomes By the end of the course, the student must be able to: Propose suitable design, production and performance criteria for the production of a composite partApply the basic equations for process and mechanical properties modelling for composite materialsDiscuss the main types of composite applications Transversal skills Use a work methodology appropriate to the task.Use both general and domain specific IT resources and toolsCommunicate effectively with professionals from other disciplines.Evaluate one's own performance in the team, receive and respond appropriately to feedback. Teaching methods Ex cathedra and invited speakers Group sessions with exercises or work on the project Expected student activities Attendance at lectures Design of a composite part, bibliography search   Assessment methods Written exam report and oral presentation in class\n"
     ]
    }
   ],
   "source": [
    "# Check the first course's description without pre-processing\n",
    "print(courses[0]['courseId'])\n",
    "print(courses[0]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, remove the stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that remove all the words in a words_list from course descriptions\n",
    "def remove(words_lst):\n",
    "    for course in courses:\n",
    "        desc_lst = course['description'].split()\n",
    "        for word in words_lst:\n",
    "            if word in desc_lst:\n",
    "                desc_lst = list(filter(lambda x: x != word, desc_lst))\n",
    "        course['description'] = ' '.join(desc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that capitalizes all the words in a words_list\n",
    "def capitalize_words(words_lst):\n",
    "    a = set([])\n",
    "    for i in words_lst:\n",
    "        a.add(i.capitalize())\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capitalized_stopwords = capitalize_words(list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the stopwords and capitalized_stopwords\n",
    "remove(list(stopwords))\n",
    "remove(list(capitalized_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the punctuation\n",
    "punctuation_table = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "for course in courses:\n",
    "    course['description'] = course['description'].translate(punctuation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE-440\n",
      "latest developments processing generations organic composites discussed Nanocomposites adaptive composites biocomposites presented Product development cost analysis study markets practiced team work Content Basics composite materialsConstituentsProcessing compositesDesign composite structures Current developmentNanocomposites Textile compositesBiocompositesAdaptive composites ApplicationsDriving forces marketsCost analysisAerospaceAutomotiveSport Keywords Composites  Applications  Nanocomposites  Biocomposites  Adaptive composites  Design  Cost Learning Prerequisites Required courses Notion polymers Recommended courses Polymer Composites Learning Outcomes end course student to Propose suitable design production performance criteria production composite partApply basic equations process mechanical properties modelling composite materialsDiscuss main types composite applications Transversal skills work methodology taskUse general domain specific IT resources toolsCommunicate effectively professionals disciplinesEvaluate ones performance team receive respond appropriately feedback Teaching methods cathedra invited speakers Group sessions exercises work project Expected student activities Attendance lectures Design composite part bibliography search Assessment methods Written exam report oral presentation class\n"
     ]
    }
   ],
   "source": [
    "# Check the first course's description\n",
    "print(courses[0]['courseId'])\n",
    "print(courses[0]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the frequent and infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make frequent words list and infrequent words list\n",
    "frequency_dict = {}\n",
    "for course in courses:\n",
    "    for word in course['description'].split():\n",
    "            if word in frequency_dict:\n",
    "                frequency_dict[word] += 1\n",
    "            else:\n",
    "                frequency_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency_sorted = sorted(frequency_dict.items(), key=lambda x:x[1], reverse = True)\n",
    "infrequency_sorted = sorted(frequency_dict.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('methods', 1556),\n",
       " ('Learning', 1237),\n",
       " ('student', 1177),\n",
       " ('Content', 835),\n",
       " ('course', 762),\n",
       " ('courses', 755),\n",
       " ('end', 661),\n",
       " ('students', 652),\n",
       " ('systems', 621),\n",
       " ('to', 602)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 frequent words\n",
    "frequency_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biocomposites', 1),\n",
       " ('materialsConstituentsProcessing', 1),\n",
       " ('developmentNanocomposites', 1),\n",
       " ('Textile', 1),\n",
       " ('compositesBiocompositesAdaptive', 1),\n",
       " ('ApplicationsDriving', 1),\n",
       " ('marketsCost', 1),\n",
       " ('analysisAerospaceAutomotiveSport', 1),\n",
       " ('Biocomposites', 1),\n",
       " ('partApply', 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 infrequent words\n",
    "infrequency_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent and infrequent words\n",
    "- Frequent words, like 'methods' and 'Learning', are very general and don't effect on courses' characteristics. \n",
    "- Also infrequent words, like 'biocomposites' and 'materialsConstituentsProcessing', are too specialized and not useful for characterising courses.\n",
    "\n",
    "Let's remove these words. Frequent words are not so many but dominant, so remove 0.1% of frequent words and 1% of infrequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that extracts a certain percentage of words from a words_lst.\n",
    "def extract(words_lst, percent):\n",
    "    removed = 0\n",
    "    taken_lst = []\n",
    "    while removed <= len(words_lst)*percent*0.01:\n",
    "        taken_lst.append(words_lst[removed][0])\n",
    "        removed += 1\n",
    "    return taken_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract 0.1% of frequent words and 1% of infrequent words.\n",
    "frequent_words = extract(frequency_sorted, 0.1)\n",
    "infrequent_words = extract(infrequency_sorted, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove(infrequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE-440\n",
      "latest developments processing generations organic composites discussed Nanocomposites adaptive composites presented Product development cost study markets practiced team Basics composite compositesDesign composite structures Current composites forces Composites Applications Nanocomposites Adaptive composites Design Cost Required Notion polymers Recommended Polymer Composites Propose suitable production performance criteria production composite basic equations process mechanical properties modelling composite main types composite applications Transversal methodology taskUse general domain specific IT resources toolsCommunicate effectively professionals disciplinesEvaluate ones performance team receive respond appropriately feedback cathedra invited speakers Group sessions exercises Expected Attendance lectures Design composite part bibliography search Written exam report oral presentation class\n"
     ]
    }
   ],
   "source": [
    "# Check the first course's description\n",
    "print(courses[0]['courseId'])\n",
    "print(courses[0]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem the words with nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps =PS()\n",
    "\n",
    "for course in courses:\n",
    "    desc_lst = course['description'].split()\n",
    "    for i in range(len(desc_lst)):\n",
    "        desc_lst[i] = ps.stem(desc_lst[i])\n",
    "    course['description'] = course['description'] = ' '.join(desc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE-440\n",
      "latest develop process gener organ composit discuss nanocomposit adapt composit present product develop cost studi market practic team basic composit compositesdesign composit structur current composit forc composit applic nanocomposit adapt composit design cost requir notion polym recommend polym composit propos suitabl product perform criteria product composit basic equat process mechan properti model composit main type composit applic transvers methodolog taskus gener domain specif IT resourc toolscommun effect profession disciplinesevalu one perform team receiv respond appropri feedback cathedra invit speaker group session exercis expect attend lectur design composit part bibliographi search written exam report oral present class\n"
     ]
    }
   ],
   "source": [
    "# Check the first course's description\n",
    "print(courses[0]['courseId'])\n",
    "print(courses[0]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain which ones you implemented and why.\n",
    "\n",
    "- We implemented removing stopwords, punctuation, the very frequent words, the infrequent words, the stem words .\n",
    "- Clearly, stopwords and punctuation are needless for characterizing courses.\n",
    "- Also, as we mentioned in the \"frequent and infrequent words part\", frequent words are too general and infrequent words like 'biocomposites' and 'materialsConstituentsProcessing', are too specialized and not useful for characterising courses. Therefore we removed them.\n",
    "- For the stemming and lemmatise, they are similar ones, so I chose stemming for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print the terms in the pre-processed description of the IX class in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '30', '50', 'acquir', 'ad', 'ad', 'advertis', 'algebra', 'algebra', 'algorithm', 'algorithm', 'analyt', 'analyt', 'apach', 'applic', 'applic', 'auction', 'auction', 'balanc', 'base', 'base', 'basic', 'basic', 'basic', 'cathedra', 'chain', 'class', 'class', 'class', 'cloud', 'cluster', 'cluster', 'collect', 'com300', 'combin', 'commun', 'commun', 'commun', 'comput', 'comput', 'concret', 'coverag', 'curat', 'current', 'data', 'data', 'data', 'data', 'data', 'data', 'dataset', 'dataset', 'decad', 'dedic', 'design', 'detect', 'detect', 'dimension', 'draw', 'ecommerc', 'ecommerc', 'effect', 'effici', 'etc', 'exam', 'expect', 'explor', 'explor', 'explor', 'explor', 'explor', 'field', 'final', 'foundat', 'framework', 'function', 'fundament', 'good', 'graph', 'graph', 'hadoop', 'hadoop', 'handson', 'homework', 'homework', 'import', 'inform', 'inform', 'infrastructur', 'inspir', 'internet', 'internet', 'java', 'key', 'knowledg', 'lab', 'lab', 'lab', 'laboratori', 'largescal', 'largescal', 'largescal', 'learn', 'learn', 'lectur', 'lectur', 'linear', 'linear', 'machin', 'machin', 'main', 'mapreduc', 'markov', 'materi', 'materi', 'media', 'midterm', 'mine', 'mine', 'mine', 'model', 'model', 'model', 'model', 'model', 'model', 'modelsdatamin', 'network', 'network', 'network', 'network', 'number', 'number', 'onlin', 'onlin', 'onlin', 'onlin', 'onlin', 'past', 'practic', 'practic', 'problem', 'problem', 'project', 'provid', 'question', 'real', 'realworld', 'realworld', 'realworld', 'realworld', 'recommend', 'recommend', 'recommend', 'reduct', 'relat', 'requir', 'retriev', 'search', 'searchretrievaltop', 'seek', 'selfcontain', 'servic', 'servic', 'servic', 'servicesanalyz', 'servicesdevelop', 'session', 'session', 'social', 'social', 'social', 'social', 'social', 'spark', 'specif', 'start', 'statist', 'stochast', 'stream', 'stream', 'structur', 'techniqu', 'theoret', 'theori', 'thi', 'togeth', 'topic', 'typic', 'ubiquit', 'user', 'weekli', 'world']\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in courses:\n",
    "    if i['courseId'] == 'COM-308':\n",
    "        ix_desc = i['description'].split()\n",
    "        ix_desc.sort()\n",
    "        print(ix_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix\n",
    "- computation of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of all words used in the courses' descriptions is 14513.\n"
     ]
    }
   ],
   "source": [
    "# Make a set that contains all the words in the courses.\n",
    "all_lst = []\n",
    "for course in courses:\n",
    "    for word in course['description'].split():\n",
    "        all_lst.append(word)\n",
    "all_set = set(all_lst)\n",
    "print(\"The number of all words used in the courses' descriptions is {}.\".format(len(all_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14513, 854)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the TF matrix\n",
    "TF_matrix = np.zeros((len(all_set),len(courses)))\n",
    "TF_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make indices for column (courseIds) and row (words).\n",
    "column_indices = [x['courseId'] for x in courses]\n",
    "row_indices = list(all_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, word in enumerate(row_indices):\n",
    "    for j, course in enumerate(courses):\n",
    "        desc_lst = course['description'].split()\n",
    "        occur_time = desc_lst.count(word)\n",
    "        TF_matrix[i][j] = occur_time/len(desc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDF = np.zeros(len(all_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the IDF\n",
    "for row in range(len(row_indices)):\n",
    "    course_sum = np.sum(TF_matrix[row] != 0)\n",
    "    IDF[row] = math.log(len(courses)/course_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDF = IDF.reshape(len(IDF),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.74993119],\n",
       "       [ 2.9212898 ],\n",
       "       [ 6.74993119],\n",
       "       ..., \n",
       "       [ 6.74993119],\n",
       "       [ 6.05678401],\n",
       "       [ 6.74993119]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the TFIDF_matrix\n",
    "TFIDF_matrix = TF_matrix*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14513, 854)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pkl(TFIDF_matrix, 'tfidx_matrix.pkl')\n",
    "save_pkl(column_indices, 'courses.pkl')\n",
    "save_pkl(row_indices, 'terms.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the TFIDF values of  'COM-308'\n",
    "ix_index = column_indices.index('COM-308')\n",
    "ix_word = TFIDF_matrix[:, ix_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_TFIDF_vals = {}\n",
    "for i, val in enumerate(ix_word):\n",
    "    if not val > 0.0:\n",
    "        continue\n",
    "    ix_TFIDF_vals[row_indices[i]] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Print the 15 terms in the description of the IX class with the highest TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onlin', 0.088542142062522422),\n",
       " ('realworld', 0.088394291067369318),\n",
       " ('social', 0.082031405578561131),\n",
       " ('explor', 0.078662153551405739),\n",
       " ('mine', 0.072598284747804151),\n",
       " ('hadoop', 0.062764601173353626),\n",
       " ('largescal', 0.06182397624169101),\n",
       " ('ecommerc', 0.05856289020850218),\n",
       " ('servic', 0.056182958205068703),\n",
       " ('auction', 0.055581728835944866),\n",
       " ('internet', 0.05138001787109342),\n",
       " ('network', 0.048563978166307087),\n",
       " ('data', 0.047383974717368979),\n",
       " ('dataset', 0.045098817834095334),\n",
       " ('ad', 0.043367687423078061)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the 15 terms in the description of the IX class with the highest TF-IDF scores.\n",
    "ix_TFIDF_vals_sorted = sorted(ix_TFIDF_vals.items(), key=lambda x:x[1], reverse = True)\n",
    "ix_TFIDF_vals_sorted[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain where the difference between the large scores and the small ones comes from.\n",
    "\n",
    "- Compared to the top words, like 'onlin' or 'realworld', some words like 'model' and 'data' occured in the description of IX class. However their TFIDF values are relatively small. The reason is that these words like 'model' and 'data' also occur in other courses' descriptions. Frequency for all descriptions is the defference between the large scores and the small ones comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct query vectors for markov chain and facebook\n",
    "markov_chain = np.zeros(len(row_indices)).reshape(len(row_indices),1)\n",
    "markov_chain[row_indices.index('markov')] = 1/2\n",
    "markov_chain[row_indices.index('chain')] = 1/2\n",
    "\n",
    "facebook = np.zeros(len(row_indices))\n",
    "facebook[row_indices.index('facebook')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that calculates the cosine similarity of  2 documents\n",
    "def similarity(di, dj):\n",
    "    return np.dot(di, dj) / (LA.norm(di) * LA.norm(dj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markov_similarity = []\n",
    "for i in range(len(column_indices)):\n",
    "    markov_similarity.append(similarity(TFIDF_matrix.T[i], markov_chain)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of (course name: similarity value)\n",
    "markov_sim_vals = {}\n",
    "for i, sim in enumerate(markov_similarity):\n",
    "    markov_sim_vals[courses[i]['name']] = sim\n",
    "    \n",
    "# Sort by similarity value\n",
    "markov_sim_vals_sorted = sorted(markov_sim_vals.items(), key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "facebook_similarity = []\n",
    "for i in range(len(column_indices)):\n",
    "    facebook_similarity.append(similarity(TFIDF_matrix.T[i], facebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of (course name: similarity value)\n",
    "facebook_sim_vals = {}\n",
    "for i, sim in enumerate(facebook_similarity):\n",
    "    facebook_sim_vals[courses[i]['name']] = sim\n",
    "\n",
    "# Sort by similarity value\n",
    "facebook_sim_vals_sorted = sorted(facebook_sim_vals.items(), key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Display the top five courses together with their similarity score for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top five courses together with their similarity score for query markov chain\n",
      "('Applied stochastic processes', 0.55381084208926346)\n",
      "('Applied probability & stochastic processes', 0.54293465222562776)\n",
      "('Markov chains and algorithmic applications', 0.38715760245681952)\n",
      "('Supply chain management', 0.36275654756075915)\n",
      "('Mathematical models in supply chain management', 0.2989216694626754)\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 courses\n",
    "print(\"The top five courses together with their similarity score for query markov chain\")\n",
    "for i in range(5):\n",
    "    print(markov_sim_vals_sorted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top five courses together with their similarity score for query facebook\n",
      "('Computational Social Media', 0.1762569139361605)\n",
      "('Composites technology', 0.0)\n",
      "('Image Processing for Life Science', 0.0)\n",
      "('Global business environment', 0.0)\n",
      "('Electrochemical nano-bio-sensing and bio/CMOS interfaces', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 courses\n",
    "print(\"The top five courses together with their similarity score for query facebook\")\n",
    "for i in range(5):\n",
    "    print(facebook_sim_vals_sorted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What do you think of the results? Give your intuition on what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_markov = 0\n",
    "count_chain = 0\n",
    "count_facebook = 0\n",
    "for course in courses:\n",
    "    for word in course['description'].split():\n",
    "        if word == 'markov':\n",
    "            count_markov += 1\n",
    "        if word == 'chain':\n",
    "            count_chain += 1\n",
    "        if word == 'facebook':\n",
    "            count_facebook += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of markov in descriptions is 42\n",
      "The number of chain in descriptions is 68\n",
      "The number of facebook in descriptions is 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of markov in descriptions is {}\".format(count_markov))\n",
    "print(\"The number of chain in descriptions is {}\".format(count_chain))\n",
    "print(\"The number of facebook in descriptions is {}\".format(count_facebook))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For markov chain, the suggested top five courses look very related to markov chain.For facebook, most of the simirality values of suggested top five courses are 0. This means that this model cannot detect courses that relate to facebook. \n",
    "- According to the numbers mentioned above, facebook doesn't occur often in the descriptions. Therefore, the result shows that this TF-IDF model can detect courses related to the words which exactly occur in the descriptions many times. However, this model can't detect courses with words that are too concrete and don't occur in the descriptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
