{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "[Spark](https://spark.apache.org/docs/1.6.2/index.html) (we are going to use version 1.6.2) is a fast and general-purpose cluster computing system, that allows to process large datasets using several machines. In the era of the *big data*, it has become a necessity to be able to distribute computing power and process data in parallel. We introduce here the basics of the framework.\n",
    "\n",
    "In any case, you should have a look at the [official tutorial](https://spark.apache.org/docs/1.6.2/programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop File System (HDFS)\n",
    "\n",
    "The main abstraction Spark offers is the *Resisilient Distributed Dataset* (RDD), which is a collection of elements distributed across the nodes of the cluster. The idea is that you typically **do not** load them in the memory, but rather process them *lazily*. They are initialized with files in the *Hadoop File System* (HDFS). \n",
    "\n",
    "Let us first see what there is in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - shirinov hdfs          0 2019-02-26 17:44 .sparkStaging\r\n",
      "-rw-r--r--   3 shirinov hdfs    2147813 2019-02-24 20:09 election-day-tweets.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* a line starting with `!` indicates a shell command. You can hence run this same command in a bash terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add a dataset to HFDS. In the `data` folder, there is text file containing 30000 tweets collected from the [Inauguration Day](https://en.wikipedia.org/wiki/United_States_presidential_inauguration) on January 20, 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `election-day-tweets.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put data/election-day-tweets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that the file is indeed in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - shirinov hdfs          0 2019-02-26 17:44 .sparkStaging\r\n",
      "-rw-r--r--   3 shirinov hdfs    2147813 2019-02-24 20:09 election-day-tweets.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* you can remove files from HDFS using `hdfs dfs -rm FILE`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset processing\n",
    "\n",
    "We can now start playing with the tweets. We will instantiate the dataset from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = sc.textFile('election-day-tweets.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tweets` variable is now an RDD. An RDD is *resilient* to faults by storing the sequence of operations applied to it so that it can easily recreate the state of the data structure in case of a problem. It can also be logically cut to be *distributed* on several machines.\n",
    "\n",
    "The `sc` object is a special object, namely a [Spark Context](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext) obeject, that we configured to be available everywhere in your Python environment. It is the main interface with Spark that has numerous useful methods. Check the documentation!\n",
    "\n",
    "Let us display a few tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: @Lawrence @HillaryClinton Two first  @SenSchumer tomorrow. @TheLastWord #brooklyn  TheRealAmerica #Vote #Democrats #NastyWomenVote #Senate\n",
      "1: My @latimesopinion op-ed on historic #California #Senate race. First time an elected woman senator succeeds another.\n",
      "2: https://t.co/cbjQTK0Q1V\n",
      "3: #Senate Wisconsin Senate Preview: Johnson vs. Feingold\n",
      "4: If Rubio Wins and #Trump Loses in #Florida... #HillaryClinton #Senate #RepublicanPrimary #Senaterace #Miami... https://t.co/zIeNEcVnMO\n",
      "5: #Senate Wisconsin Senate Preview: Johnson vs. Feingold\n",
      "6: bob day is an honest  person   #senate patterson . a loss to the senate\n",
      "7: Make Republicans #PayAPrice!\n",
      "8:  ðŸ’™ðŸ‡ºðŸ‡¸#VoteBLUEðŸ”ƒtheBallotðŸ‡ºðŸ‡¸ðŸ’™\n",
      "9:  #Congress #Senate #FlipItDem\n"
     ]
    }
   ],
   "source": [
    "some_tweets = tweets.take(10)\n",
    "for i, tweet in enumerate(some_tweets):\n",
    "    print(\"%d: %s\" % (i, tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the `tweets` dataset can be acted on by dataset **operations**. RDDs support two types of operations: *transformations*, which create a new RDD from an existing one, and *actions*, which return a value to the driver program after running a computation on the RDD. For instance, we can `count` the number of tweets. Is it a transofrmation or an action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also `filter` the lines matching a specific string. Is it a transformation or an action ?\n",
    "\n",
    "*Note*: `lower()` is a `str` method that converts the string to lower case. What happens if you do not do that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('in' in 'this is how to use in')\n",
    "lines_with_trump = tweets.filter(lambda line: \"trump\" in line.lower())\n",
    "lines_with_trump.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical processing pattern is [MapReduce](https://en.wikipedia.org/wiki/MapReduce), as popularized by Hadoop. The idea is that you apply a function to your dataset that filters or sorts it (**map**), and then combine the ouputs by computing something (**reduce**). The typical example is a word count. So, how many times each word occur among our tweets?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lawrence',\n",
       " '@HillaryClinton',\n",
       " 'Two',\n",
       " 'first',\n",
       " '@SenSchumer',\n",
       " 'tomorrow.',\n",
       " '@TheLastWord',\n",
       " '#brooklyn',\n",
       " 'TheRealAmerica',\n",
       " '#Vote',\n",
       " '#Democrats',\n",
       " '#NastyWomenVote',\n",
       " '#Senate',\n",
       " 'My',\n",
       " '@latimesopinion']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split every line into words and flattens the result (aggregate them all together)\n",
    "words = tweets.flatMap(lambda line: line.split())\n",
    "words.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@Lawrence', 1),\n",
       " ('@HillaryClinton', 1),\n",
       " ('Two', 1),\n",
       " ('first', 1),\n",
       " ('@SenSchumer', 1),\n",
       " ('tomorrow.', 1),\n",
       " ('@TheLastWord', 1),\n",
       " ('#brooklyn', 1),\n",
       " ('TheRealAmerica', 1),\n",
       " ('#Vote', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign 1 to each word\n",
    "counts = words.map(lambda word: (word, 1))\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@HillaryClinton', 121),\n",
       " ('Two', 8),\n",
       " ('#Democrats', 20),\n",
       " ('op-ed', 1),\n",
       " ('historic', 10),\n",
       " ('race.', 5),\n",
       " ('an', 316),\n",
       " ('senator', 10),\n",
       " ('another.', 3),\n",
       " ('https://t.co/cbjQTK0Q1V', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combines the counts by adding up the value (1) of equal keys (words)\n",
    "word_counts = counts.reduceByKey(lambda a, b: a + b)\n",
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read the list of common [transformations](https://spark.apache.org/docs/1.6.2/programming-guide.html#transformations) and [actions](https://spark.apache.org/docs/1.6.2/programming-guide.html#actions) to get an idea of what you can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics\n",
    "\n",
    "Your turn! Try to extract some statisctis from the dataset:\n",
    "\n",
    "1. Total number of words\n",
    "2. Average number of words per tweet\n",
    "3. Ten most frequent word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "A bigram is combination of two consecutive words. \n",
    "\n",
    "- Extract the ten most frequent bigrams from the dataset. \n",
    "\n",
    "Be careful not to consider the last word of a tweet and the first one of the next tweet!\n",
    "\n",
    "*Hint:* note that the `map` and `flatMap` methods take a **function** as argument, meaning that you can define a function that you pass as argument:\n",
    "\n",
    "```python\n",
    "def process(line):\n",
    "    # Do something\n",
    "    return something\n",
    "\n",
    "x = tweets.flatMap(process)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('! #BlackMoney', 1),\n",
       " ('! #Election2016', 1),\n",
       " ('! ...and', 1),\n",
       " ('! 1471', 1),\n",
       " ('! 1500', 1),\n",
       " ('! @RBI', 1),\n",
       " ('! @Wade4Justice', 1),\n",
       " ('! A', 1),\n",
       " ('! Aaj', 1),\n",
       " ('! Am', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "def make_bigrams(line): \n",
    "    words = line.split()\n",
    "    bigrams = ['%s %s' % (words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "    return bigrams\n",
    "\n",
    "bigrams = tweets.flatMap(make_bigrams)\n",
    "bigrams.take(100)\n",
    "counts = bigrams.map(lambda word: (word, 1))\n",
    "counts.take(10)\n",
    "# # Combines the counts by adding up the value (1) of equal keys (bigrams)\n",
    "bigrams_counts = counts.reduceByKey(lambda a, b: a + b)\n",
    "bigrams_counts.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
